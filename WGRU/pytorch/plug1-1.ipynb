{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Load and process the data\n",
    "aggregate_df1 = pd.read_csv('/home/awinlab/Documents/kevin/elec110.csv', index_col='datetime', parse_dates=True)\n",
    "aggregate_df1 = aggregate_df1[:900000]\n",
    "\n",
    "plug1_1 = pd.read_csv('/home/awinlab/Documents/kevin/plug1-1.csv', index_col='datetime', parse_dates=True)\n",
    "plug1_1 = plug1_1[:900000]\n",
    "\n",
    "aggregate_df1 = aggregate_df1.drop(['id','device', 'vo', 'cu', 'reactive', 'apparent', 'pf', 'freq'], axis=1)\n",
    "plug1_1 = plug1_1.drop(['id','device', 'vo', 'cu', 'reactive', 'apparent', 'pf', 'freq'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, length=100):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) - self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X[idx: idx + self.length]\n",
    "        y = self.y[idx: idx + self.length]\n",
    "        return X, y\n",
    "\n",
    "mask = [i % 5 != 0 for i in range(len(aggregate_df1))]\n",
    "X_train = aggregate_df1[~np.array(mask)]\n",
    "X_test = aggregate_df1[mask]\n",
    "\n",
    "\n",
    "\n",
    "mask1 = [i % 5 != 0 for i in range(len(plug1_1))]\n",
    "y_train = plug1_1[~np.array(mask1)]\n",
    "y_test = plug1_1[mask1]\n",
    "\n",
    "\n",
    "X_test = pd.DataFrame(X_test)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train.values).float()\n",
    "X_test = torch.tensor(X_test.values).float()\n",
    "y_train = torch.tensor(y_train.values).float()\n",
    "y_test = torch.tensor(y_test.values).float()\n",
    "\n",
    "batch_size=60\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train, y_train, length=100)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers = 0, drop_last = True, \n",
    "                                           collate_fn=lambda x: (torch.nn.utils.rnn.pad_sequence([i[0] for i in x], batch_first=True),\n",
    "                                                                 torch.nn.utils.rnn.pad_sequence([i[1] for i in x], batch_first=True)))\n",
    "\n",
    "test_dataset = TimeSeriesDataset(X_test, y_test, length=100)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 0, drop_last = True, \n",
    "                                           collate_fn=lambda x: (torch.nn.utils.rnn.pad_sequence([i[0] for i in x], batch_first=True),\n",
    "                                                                 torch.nn.utils.rnn.pad_sequence([i[1] for i in x], batch_first=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rete(y_true, y_pred):\n",
    "    y_true = y_true.view(-1, 1)\n",
    "    y_pred = y_pred.view(-1, 1)\n",
    "\n",
    "    # Compute the maximum value between y_true and y_pred element-wise, and reshape it to have shape (batch_size*seq_len, 1)\n",
    "    max_val, _ = torch.max(torch.cat([y_pred, y_true], dim=1), dim=1, keepdim=True)\n",
    "\n",
    "    # Compute the relative error between y_true and y_pred element-wise, and reshape it to have shape (batch_size*seq_len, 1)\n",
    "    rel_err = torch.abs(y_pred - y_true) / max_val\n",
    "\n",
    "    # Take the mean of the relative error across all samples in the batch\n",
    "    return torch.mean(rel_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN version incompatibility: PyTorch was compiled  against (8, 5, 0) but found runtime version (8, 0, 1). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 5, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b386bb659ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mmae_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmae_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/daniel_flower/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/anaconda3/envs/daniel_flower/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/daniel_flower/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_weights_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# Flattens params (on CUDA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/daniel_flower/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mflatten_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m             if (not isinstance(fw.data, Tensor) or not (fw.data.dtype == dtype) or\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32mnot\u001b[0m \u001b[0mfw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                     not torch.backends.cudnn.is_acceptable(fw.data)):\n\u001b[0m\u001b[1;32m    160\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/daniel_flower/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36mis_acceptable\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \"PyTorch making sure the library is visible to the build system.\")\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         warnings.warn('cuDNN/MIOpen library not found. Check your {libpath}'.format(\n\u001b[1;32m     97\u001b[0m             libpath={\n",
      "\u001b[0;32m~/anaconda3/envs/daniel_flower/lib/python3.7/site-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mld_library_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LD_LIBRARY_PATH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mld_library_path\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubstring\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cudnn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                         raise RuntimeError(f'{base_error_msg}'\n\u001b[0m\u001b[1;32m     51\u001b[0m                                            \u001b[0;34mf'Looks like your LD_LIBRARY_PATH contains incompatible version of cudnn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                                            f'Please either remove it from the path or install cudnn {compile_version}')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuDNN version incompatibility: PyTorch was compiled  against (8, 5, 0) but found runtime version (8, 0, 1). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 5, 0)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_shape, n_input):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv0 = nn.Conv1d(100, 16, kernel_size=1, stride=1, padding=1)\n",
    "        self.conv1 = nn.Conv1d(16, 4, kernel_size=1, stride=1, padding=1)\n",
    "        self.gru0 = nn.GRU(5, 64, num_layers=2, bidirectional=True)\n",
    "        self.gru1 = nn.GRU(128, 128, num_layers=2, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(256, 256, num_layers=2, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(512, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x, _ = self.gru0(x)\n",
    "\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "n_input = 100\n",
    "input_shape = (n_input, 1)\n",
    "model = Model(input_shape, n_input)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08)\n",
    "mae_fn = nn.L1Loss()\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import L1Loss\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "mae_fn = mae_fn.to(device)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "        for i, (X_batch, y_batch) in enumerate(tqdm.tqdm(train_loader)):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_pred = model(X_batch) \n",
    "            loss = loss_fn(y_pred.view(-1), y_batch.view(-1,1))\n",
    "            mae = mae_fn(y_pred.view(-1), y_batch.view(-1,1))\n",
    "            rete_val = rete(y_batch[:,0,:].view(-1,1), y_pred.view(-1,1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        tqdm.tqdm.write(f\"Epoch [{epoch+1}/{epochs}], Step [{i}/{len(train_loader)}], Loss: {loss.item()}, MAE: {mae.item()}, Rete: {rete_val.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = []\n",
    "model = model.cpu()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        predict.append(outputs[:, 0].unsqueeze(1))\n",
    "predict = torch.cat(predict, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_list = np.concatenate([p.detach().numpy() for p in predict], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(facecolor='white')\n",
    "plt.plot(predict_list,color='darkorange')\n",
    "plt.plot(y_test)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(\"W\")\n",
    "plt.title(\"plug1-1\")\n",
    "plt.savefig('/home/awinlab/Documents/kevin/WGRU/pytorch/WindowGRU_plug1-1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daniel_flower",
   "language": "python",
   "name": "daniel_flower"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
